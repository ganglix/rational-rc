<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>Tinkrete.helper_func API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Tinkrete.helper_func</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import logging

# Declare first, as it provides global default value for helper functions
N_SAMPLE = int(1e5)


# logger
# log levels: NOTSET, DEBUG, INFO, WARNING, ERROR, and CRITICAL
LOG_FORMAT = &#34;%(levelname)s %(asctime)s - %(message)s&#34;
logging.basicConfig(
    filename=&#34;mylog.log&#34;,
    # level=logging.DEBUG,
    format=LOG_FORMAT,
)

logger = logging.getLogger(__name__)
logger.setLevel(
    logging.CRITICAL
)  # set logging level here to work in jupyter notebook where maybe a default setting was there


# master helper functions

# Helper function
def dropna(x):
    return x[~np.isnan(x)]


def Get_mean(x):
    &#34;&#34;&#34;get mean ignoring nans&#34;&#34;&#34;
    x = x[~np.isnan(x)]  # remove nans
    return x.mean()


def Get_std(x):
    &#34;&#34;&#34;get standard deviation ignoring nans&#34;&#34;&#34;
    x = x[~np.isnan(x)]  # remove nans
    return x.std()


def Hist_custom(S):
    &#34;&#34;&#34;plot histogram with N_SAMPLE//100 bins ignoring nans&#34;&#34;&#34;
    S_dropna = S[~np.isnan(S)]
    fig, ax = plt.subplots()
    ax.hist(
        S_dropna, bins=min(N_SAMPLE // 100, 100), density=True, alpha=0.5, color=&#34;C0&#34;
    )


# Sampler updated
def Normal_custom(m, s, n_sample=N_SAMPLE, non_negative=False, plot=False):
    &#34;&#34;&#34; Sampling from a normal distribution

    Parameters
    ----------
    m : int or float
        mean
    s : int or float
        standard deviation
    n_sample : int
        sample number, default is a Global var N_SAMPLE
    non_negative: bool
        if true, return truncated distribution with no negatives, default is False
    plot : bool
        default is False

    Returns
    -------
    out : numpy array
    &#34;&#34;&#34;
    x = np.random.normal(loc=m, scale=s, size=n_sample)
    if non_negative:
        x = stats.truncnorm.rvs(
            (0 - m) / s, (np.inf - m) / s, loc=m, scale=s, size=n_sample
        )
    if plot:
        fig, ax = plt.subplots()
        ax.hist(x)
        plt.show()
    return x


def Beta_custom(m, s, a, b, n_sample=N_SAMPLE, plot=False):
    &#34;&#34;&#34; draw samples from a general beta distribution described by mean, std and lower and upper   bounds
    X~General Beta(a,b, loc = c, scale = d)
    Z~std Beta(alpha, beta)

    X = c + d*Z
    E(X) = c + d * E(Z)
    var(X) = d^2 * var(Z)

    Parameters
    ----------
    m : mean
    s : standard deviation
    a : lower bound, not shape param a(alpha)
    b : upper bound, not shape param b(beta)
    n_sample: int
        sample number
    plot : bool
        default is False

    Returns
    -------
    out : numpy array
    &#34;&#34;&#34;
    # location:c and scale:d for General Beta (standard Beta range [0,1])
    c = a
    d = b - a

    # mean and variance for
    mu = (m - c) / d
    var = s ** 2 / d ** 2

    # shape params for Z~standard beta
    alpha = ((1 - mu) / var - 1 / mu) * mu ** 2
    beta = alpha * (1 / mu - 1)
    z = np.random.beta(alpha, beta, size=n_sample)

    # transfer back to General Beta
    x = c + d * z

    if plot:
        fig, ax = plt.subplots()
        ax.hist(x)
        print(x.mean(), x.std())
        plt.show()
    return x


def interp_extrap_f(x, y, x_find, plot=False):
    &#34;&#34;&#34;interpolate or extrapolate value from an array with fitted2-deg or 3-deg polynomial

    Parameters
    ----------
    x : array-like
        varible
    y : array-like
        function value
    x_find : int or float or array-like
        look-up x
    plot : bool
        plot curve fit and data points, default if false

    Returns
    -------
    int or float or array-like
        inter/extrapolated value(s), raise warning when extrapolation is used
    &#34;&#34;&#34;

    def func2(x, a, b, c):
        # 2-order polynomial
        return a * (x ** 2) + b * (x ** 1) + c

    def func3(x, a, b, c, d):
        # 3-order polynomial
        return a * (x ** 2) + b * (x ** 2) + c * x + d

    if (x_find &lt; x.min()).any() or (x_find &gt; x.max()).any():
        logger.warning(&#34;Warning: extrapolation used&#34;)

    from scipy.optimize import curve_fit

    # Initial parameter guess, just to kick off the optimization
    if len(y) &gt; 3:
        logger.debug(&#34;use func3: 3-deg polynomial&#34;)
        guess = (0.5, 0.5, 0.5, 0.5)
        popt, _ = curve_fit(func3, x, y, p0=guess)
        y_find = func3(x_find, *popt)
        if plot:
            fig, ax = plt.subplots()
            ax.plot(x, y, &#34;.&#34;, label=&#34;table&#34;)
            _plot_data = np.linspace(x.min(), x.max(), 100)
            ax.plot(_plot_data, func3(_plot_data, *popt), &#34;--&#34;)
            ax.plot(
                x_find, y_find, &#34;x&#34;, color=&#34;r&#34;, markersize=8, label=&#34;interp/extrap data&#34;
            )
            ax.legend()
            plt.show()

    elif len(y) &lt;= 3:
        logger.debug(&#34;use func2: 2-deg polynomial&#34;)
        guess = (0.5, 0.5, 0.5)
        popt, _ = curve_fit(func2, x, y, p0=guess)
        y_find = func2(x_find, *popt)
        if plot:
            fig, ax = plt.subplots()
            ax.plot(x, y, &#34;.&#34;, label=&#34;table&#34;)
            _plot_data = np.linspace(x.min(), x.max(), 100)
            ax.plot(_plot_data, func2(_plot_data, *popt), &#34;--&#34;)
            ax.plot(
                x_find, y_find, &#34;x&#34;, color=&#34;r&#34;, markersize=8, label=&#34;interp/extrap data&#34;
            )
            ax.legend()
            plt.show()
    else:
        y_find = None
    return y_find


def find_similar_group(item_list, similar_group_size=2):
    &#34;&#34;&#34;find similar sublist of similar_group_size from a item_list&#34;&#34;&#34;
    from itertools import combinations

    combos = np.array(list(combinations(item_list, similar_group_size)))
    ind_min = combos.std(axis=1).argmin()
    similar_group = combos[ind_min].tolist()
    return similar_group


def sample_integral(Y, x):
    &#34;&#34;&#34;integrate Y over x, where every Y point is a bunch of distribution samples,

    Parameters
    ----------
    Y : numpy array
        2D
        column: y data points
        row: samples for y point
    x : numpy array

    Returns
    -------
    int_y_x : numpy array
            integral of y over x for all sampled data
    &#34;&#34;&#34;
    from scipy.integrate import simps

    n, _ = Y.shape
    if n != len(x):
        raise Exception(&#34;Y does not have the same number of data points as x&#34;)
    int_y_x = simps(Y, x, axis=0)
    return int_y_x


def f_solve_poly2(a, b, c):
    r1 = (-b + (b ** 2 - 4 * a * c) ** 0.5) / (2 * a)
    r2 = (-b - (b ** 2 - 4 * a * c) ** 0.5) / (2 * a)
    return r1, r2


# helper function
def Fit_distrib(s, fit_type=&#34;kernel&#34;, plot=False, xlabel=&#34;&#34;, title=&#34;&#34;, axn=None):
    &#34;&#34;&#34;fit data to a probability distribution function(parametric or numerical)
    and return a continuous random variable or a random variable represented by Gaussian kernels
    parametric : normal
    numerical : Gaussian kernels

    Parameters
    ----------
    s : array-like
        sample data
    fit_type : string
        fit type keywords, &#39;kernel&#39;, &#39;normal&#39;
    plot : bool
        create a plot with histogram and fitted pdf curve

    Returns
    -------
    out : continuous random variable : stats.norm(loc = mu, scale = sigma)
              when parametric normal is used
          Gaussian kernel random variable : (stats.gaussian_kde)
              when kernel is used
    &#34;&#34;&#34;
    mu = None
    sigma = None
    kde = None
    if fit_type == &#34;normal&#34;:
        # parametric, fit normal distribution
        logger.debug(&#34;parametric, fit normal distribution&#34;)

        # Fit a curve to the variates  mu is loc sigma is scale
        mu, sigma = stats.norm.fit(s, floc=s.mean())

    elif fit_type == &#34;kernel&#34;:
        # non-parametric, this creates the kernel, given an array it will estimate the probability over that values
        logger.debug(&#34;non-parametric kernel fit&#34;)
        s_dropna = s[~np.isnan(s)]  # remove nans
        # bandwidth selection:  gaussian_kde uses a rule of thumb, the default is Scott’s Rule.
        kde = stats.gaussian_kde(s_dropna)
    else:
        raise Exception(&#34;fit_type is not set correctly&#34;)

    if plot:
        if axn is None:
            axn = plt.gca()
        n = min(len(s) // 100, 100)  # bin size
        dist_space = np.linspace(min(s), max(s), 100)
        axn.hist(s, bins=n, density=True)

        # plot pdf
        if fit_type == &#34;normal&#34;:
            # probability distribution
            pdf = stats.norm.pdf(dist_space, mu, sigma)
            axn.plot(dist_space, pdf, label=&#34;normal&#34;)

        if fit_type == &#34;kernel&#34;:
            pdf_kde = kde(dist_space)
            axn.plot(dist_space, pdf_kde, label=&#34;kernel&#34;)

        axn.set_xlabel(xlabel)
        axn.set_ylabel(&#34;distribution density&#34;)
        axn.legend(loc=&#34;upper right&#34;)
        axn.set_title(title)
    # return
    if fit_type == &#34;normal&#34;:
        return stats.norm(loc=mu, scale=sigma)
    if fit_type == &#34;kernel&#34;:
        return kde


def Pf_RS(R_info, S, R_distrib_type=&#34;normal&#34;, plot=False):  # updated!
    &#34;&#34;&#34;Calculate the probability of failure  Pf = P(R-S&lt;0), given the R(resistance) and S(load)
       with three three methods and use method 3) if checked OK with the other two
           1) crude monte carlo
           2) numerical integral of g kernel fit
           3) R S integral: (&#39;$\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$&#39;)
       reliability index(beta factor) is calculated with simple 1st order g.mean()/g.std()

    Parameters
    ----------
    R_info : tuple or numpy array
             distribution of Resistance, e.g. cover thickness, critical chloride content, tensile strength
             can be array or distribution parameters
             R_distrib_type=&#39;normal&#39; -&gt; tuple(m,s) for normal m: mean s: standard deviation
             R_distrib_type=&#39;normal&#39; -&gt; tuple(m,s,a,b) for (General) beta distribution
                             m: mean, s: standard deviation a,b : lower, upper bound
             R_distrib_type=&#39;normal&#39; -&gt; array: for not-determined distribution, will be treated numerically(R S integral is not applied )

    S : numpy array
        distribution of load, e.g. carbonation depth, chloride content, tensile stress
        the distribution type is calculated S is usually not determined, can vary a lot in different cases, therefore fitted with kernel

    R_distrib_type : string
        &#39;normal&#39;, &#39;beta&#39;, &#39;array&#39;

    Returns
    -------
    out = tuple
        (probability of failure, reliability index)

    Notes
    -----
    For R as arrays R S integral is not applied
    R S integration method: $P_f = P(R-S&lt;=0)=\int\limits_{-\infty}^{\infty}f_S(y) \int\limits_{-\infty}^{y}f_R(x)dxdy$
    the dual numerical integration seems too computationally expensive, so consider fit R to analytical distribution in the future versions

    &#34;&#34;&#34;
    from scipy import integrate

    R, pf_RS = (None, None)

    S_kde_fit = Fit_distrib(S, fit_type=&#34;kernel&#34;)
    S_dropna = S[~np.isnan(S)]

    if R_distrib_type == &#34;normal&#34;:
        # R = (mu, std)
        (m, s) = R_info
        R_distrib = stats.norm(m, s)
        R = R_distrib.rvs(size=N_SAMPLE)

        # Calculate probablility of failure
        #     $P_f = P(R-S&lt;=0)=\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$
        pf_RS = integrate.quad(
            lambda x: R_distrib.cdf(x) * S_kde_fit(x)[0], 0, S_dropna.max()
        )[0]

    elif R_distrib_type == &#34;beta&#34;:
        # R = (m, s, a, b) a, b are lower and upper bound
        (m, s, a, b) = R_info

        # location:c and scale:d for General Beta (standard Beta range [0,1])
        # calculate loc and scale
        c = a
        d = b - a

        # mean and variance for
        mu = (m - c) / d
        var = s ** 2 / d ** 2

        # shape params for Z~standard beta
        alpha = ((1 - mu) / var - 1 / mu) * mu ** 2
        beta = alpha * (1 / mu - 1)

        R_distrib = stats.beta(alpha, beta, c, d)
        R = R_distrib.rvs(size=N_SAMPLE)

        # Calculate probability of failure
        #     $P_f = P(R-S&lt;=0)=\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$
        pf_RS = integrate.quad(
            lambda x: R_distrib.cdf(x) * S_kde_fit(x)[0], 0, S_dropna.max()
        )[0]

    elif R_distrib_type == &#34;array&#34;:
        # dual numerical integration is too expensive, consider fit R to analytical distribution!!!!!!!!!!!!!!!!!!!!!!!!!!
        # plot condition to be fixed !!!!!!!!!!!!!!!!!!!!!!

        #         # use R array
        #         R_kde_fit = Fit_distrib(R, fit_type=&#39;kernel&#39;)
        #         R_dropna = R[~np.isnan(R)]
        #         # $P_f = P(R-S&lt;=0)=\int\limits_{-\infty}^{\infty}f_S(y) \int\limits_{-\infty}^{y}f_R(x)dxdy$

        #         def R_cdf_S_pdf(x, R_kde_fit, S_kde_fit):
        #             R_cdf = integrate.quad(lambda z: R_kde_fit(z)[0],0,x)[0] # kde_fit returns ([array needed]). therefore use lamda z kde(z)[0]
        #             S_pdf = S_kde_fit(x)[0]
        #             return R_cdf*S_pdf

        #         pf_RS = integrate.quad(R_cdf_S_pdf,0,S_dropna.max(), args=(R_kde_fit, S_kde_fit))[0]
        R_distrib = None
    else:
        R_distrib = None
        pass

    # compare with
    # numerical g
    g = R - S
    g = g[~np.isnan(g)]
    # numerical kernel fit
    g_kde_fit = Fit_distrib(g, fit_type=&#34;kernel&#34;, plot=False)
    pf_kde = integrate.quad(g_kde_fit, g.min(), 0)[0]
    pf_sample = len(g[g &lt;= 0]) / len(g)
    beta_factor = g.mean() / g.std()  # first order

    # check for tiny tail
    if pf_sample &lt; 1e-10:
        print(&#34;warning: very small Pf &#34;)
        logger.warning(&#34;warning: very small Pf &#34;)

    # check if pf_RS is the pf (should be)
    best_2_of_3 = find_similar_group([pf_sample, pf_kde, pf_RS], similar_group_size=2)
    if pf_RS not in best_2_of_3:
        logger.warning(&#34;warning: pf_RS is not used, double check&#34;)
        logger.warning(
            &#34;Pf(g = R-S &lt; 0) from various methods\n    sample count: {}\n    g integral: {}\n    R S integral: {}\n    beta_factor: {}&#34;.format(
                pf_sample, pf_kde, pf_RS, beta_factor
            )
        )

    logger.info(
        &#34;Pf(g = R-S &lt; 0) from various methods\n    sample count: {}\n    g integral: {}\n    R S integral: {}\n    beta_factor: {}&#34;.format(
            pf_sample, pf_kde, pf_RS, beta_factor
        )
    )

    if plot:
        print(&#34;Pf(g = R-S &lt; 0) from various methods&#34;)
        print(&#34;    sample count: {}&#34;.format(pf_sample))
        print(&#34;    g integral: {}&#34;.format(pf_kde))
        print(&#34;    R S integral: {}&#34;.format(pf_RS))
        # printmd(&#39;$\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$&#39;)
        print(&#34;    beta_factor: {}&#34;.format(beta_factor))

        # Plot R S
        fig, [ax1, ax2] = plt.subplots(ncols=2, figsize=(10, 3))
        # R
        R_plot = np.linspace(R.min(), R.max(), 100)
        ax1.plot(R_plot, R_distrib.pdf(R_plot), color=&#34;C0&#34;)
        ax1.hist(
            R,
            bins=min(N_SAMPLE // 100, 100),
            density=True,
            alpha=0.5,
            color=&#34;C0&#34;,
            label=&#34;R&#34;,
        )

        # S
        S_plot = np.linspace(S_dropna.min(), S_dropna.max(), 100)
        ax1.plot(S_plot, S_kde_fit(S_plot), color=&#34;C1&#34;, alpha=1)
        ax1.hist(
            S_dropna,
            bins=min(N_SAMPLE // 100, 100),
            density=True,
            alpha=0.5,
            color=&#34;C1&#34;,
            label=&#34;S&#34;,
        )

        ax1.set_title(
            &#34;S: mean = {:.1f} stdev = {:.1f}&#34;.format(S_dropna.mean(), S_dropna.std())
        )
        ax1.legend()
        plt.tight_layout()

        # plot g
        g_plot = np.linspace(g.min(), g.max(), 100)
        ax2.plot(g_plot, g_kde_fit(g_plot), color=&#34;C2&#34;, alpha=1)

        ax2.hist(
            g,
            density=True,
            bins=min(N_SAMPLE // 100, 100),
            color=&#34;C2&#34;,
            alpha=0.5,
            label=&#34;g=R-S&#34;,
        )
        ax2.vlines(x=0, ymin=0, ymax=g_kde_fit(0)[0], linestyles=&#34;--&#34;, alpha=0.5)
        ax2.vlines(
            x=g.mean(), ymin=0, ymax=g_kde_fit(g.mean())[0], linestyles=&#34;--&#34;, alpha=0.5
        )
        #         ax.annotate(s=&#39;&#39;, xy=(0,g_kde_fit(0)[0]), xytext=(g.mean(),g_kde_fit(0)[0]),
        #                     arrowprops={&#39;arrowstyle&#39;: &#39;&lt;-&gt;&#39;},va=&#39;center&#39;)
        ax2.annotate(
            s=r&#34;${\mu}_g$&#34;,
            xy=(0, g.mean()),
            xytext=(g.mean(), g_kde_fit(0)[0]),
            va=&#34;center&#34;,
        )
        ax2.legend()
        ax2.set_title(&#34;Limit-state P(g&lt;0)={}&#34;.format(pf_RS))
        plt.show()

    return pf_RS, beta_factor, R_distrib, S_kde_fit


def RS_plot(model, ax=None, t_offset=0, amplify=1):  # updated!
    &#34;&#34;&#34;plot R S distribution vertically at a time to an axis

    Parameters
    ----------
    model.R_distrib : scipy.stats._continuous_distns, normal or beta
                      calculated in Pf_RS() through model.postproc()
    model.S_kde_fit : stats.gaussian_kde
                      calculated in Pf_RS() through model.postproc()
                      distribution of load, e.g. carbonation depth, chlride content, tensile     stress. The distrubtion type is calculated S is usually not determined, can vary a lot in different cases, therefore fitted with kernel

    model.S : numpy array
              load, e.g. carbonation depth, chloride content, tensile stress
    ax : axis
    t_offset : time offset to move the plot along the t-axis. default is zero
    amplify : scale the height of the pdf plot
    &#34;&#34;&#34;

    R_distrib = model.R_distrib
    S_kde_fit = model.S_kde_fit
    S = model.S

    S_dropna = S[~np.isnan(S)]
    # Plot R S
    R = R_distrib.rvs(size=N_SAMPLE)

    if ax is None:
        ax = plt.gca()
    # R
    R_plot = np.linspace(R.min(), R.max(), 100)
    ax.plot(R_distrib.pdf(R_plot) * amplify + t_offset, R_plot, color=&#34;C0&#34;)
    ax.fill_betweenx(
        R_plot,
        t_offset,
        R_distrib.pdf(R_plot) * amplify + t_offset,
        color=&#34;C0&#34;,
        alpha=0.5,
        label=&#34;R&#34;,
    )
    # S  avoid ploting large S with very small probability
    S_plot = np.linspace(S_dropna.min(), min(5 * S_dropna.mean(), S_dropna.max()), 100)
    ax.plot(S_kde_fit(S_plot) * amplify + t_offset, S_plot, color=&#34;C1&#34;, alpha=1)
    ax.fill_betweenx(
        S_plot,
        t_offset,
        S_kde_fit(S_plot) * amplify + t_offset,
        color=&#34;C1&#34;,
        alpha=0.5,
        label=&#34;S&#34;,
    )


# helper function
def find_mean(val, s, confidence_one_tailed=0.95):
    &#34;&#34;&#34;return the mean value of a unknown normal distribution
    based on the given value at a known one-tailed confidence level(default 95%)

    Parameters
    ----------
    val : float
         cut-off value
    s : standard deviation
    confidence_one_tailed : confidence level

    Returns
    -------
    mean : mean value of the unknown distribution
    &#34;&#34;&#34;

    def func(m, s, val, confidence_one_tailed):
        &#34;&#34;&#34;object function to be solved&#34;&#34;&#34;
        norm = stats.norm(m, s)
        cutoff = norm.cdf(val)
        return cutoff - (1 - confidence_one_tailed)

    from scipy.optimize import fsolve

    mean = fsolve(func, x0=val, args=(s, val, confidence_one_tailed))[
        0
    ]  # use val as initial guess
    return mean</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="Tinkrete.helper_func.Beta_custom"><code class="name flex">
<span>def <span class="ident">Beta_custom</span></span>(<span>m, s, a, b, n_sample=100000, plot=False)</span>
</code></dt>
<dd>
<div class="desc"><p>draw samples from a general beta distribution described by mean, std and lower and upper
bounds
X~General Beta(a,b, loc = c, scale = d)
Z~std Beta(alpha, beta)</p>
<p>X = c + d*Z
E(X) = c + d * E(Z)
var(X) = d^2 * var(Z)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>mean</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>s</code></strong> :&ensp;<code>standard deviation</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>a</code></strong> :&ensp;<code>lower bound, not shape param a(alpha)</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>b</code></strong> :&ensp;<code>upper bound, not shape param b(beta)</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>n_sample</code></strong> :&ensp;<code>int</code></dt>
<dd>sample number</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code></dt>
<dd>default is False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Beta_custom(m, s, a, b, n_sample=N_SAMPLE, plot=False):
    &#34;&#34;&#34; draw samples from a general beta distribution described by mean, std and lower and upper   bounds
    X~General Beta(a,b, loc = c, scale = d)
    Z~std Beta(alpha, beta)

    X = c + d*Z
    E(X) = c + d * E(Z)
    var(X) = d^2 * var(Z)

    Parameters
    ----------
    m : mean
    s : standard deviation
    a : lower bound, not shape param a(alpha)
    b : upper bound, not shape param b(beta)
    n_sample: int
        sample number
    plot : bool
        default is False

    Returns
    -------
    out : numpy array
    &#34;&#34;&#34;
    # location:c and scale:d for General Beta (standard Beta range [0,1])
    c = a
    d = b - a

    # mean and variance for
    mu = (m - c) / d
    var = s ** 2 / d ** 2

    # shape params for Z~standard beta
    alpha = ((1 - mu) / var - 1 / mu) * mu ** 2
    beta = alpha * (1 / mu - 1)
    z = np.random.beta(alpha, beta, size=n_sample)

    # transfer back to General Beta
    x = c + d * z

    if plot:
        fig, ax = plt.subplots()
        ax.hist(x)
        print(x.mean(), x.std())
        plt.show()
    return x</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.Fit_distrib"><code class="name flex">
<span>def <span class="ident">Fit_distrib</span></span>(<span>s, fit_type='kernel', plot=False, xlabel='', title='', axn=None)</span>
</code></dt>
<dd>
<div class="desc"><p>fit data to a probability distribution function(parametric or numerical)
and return a continuous random variable or a random variable represented by Gaussian kernels
parametric : normal
numerical : Gaussian kernels</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>s</code></strong> :&ensp;<code>array-like</code></dt>
<dd>sample data</dd>
<dt><strong><code>fit_type</code></strong> :&ensp;<code>string</code></dt>
<dd>fit type keywords, 'kernel', 'normal'</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code></dt>
<dd>create a plot with histogram and fitted pdf curve</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>continuous random variable : stats.norm(loc = mu, scale = sigma)</code></dt>
<dd>when parametric normal is used
Gaussian kernel random variable : (stats.gaussian_kde)
when kernel is used</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Fit_distrib(s, fit_type=&#34;kernel&#34;, plot=False, xlabel=&#34;&#34;, title=&#34;&#34;, axn=None):
    &#34;&#34;&#34;fit data to a probability distribution function(parametric or numerical)
    and return a continuous random variable or a random variable represented by Gaussian kernels
    parametric : normal
    numerical : Gaussian kernels

    Parameters
    ----------
    s : array-like
        sample data
    fit_type : string
        fit type keywords, &#39;kernel&#39;, &#39;normal&#39;
    plot : bool
        create a plot with histogram and fitted pdf curve

    Returns
    -------
    out : continuous random variable : stats.norm(loc = mu, scale = sigma)
              when parametric normal is used
          Gaussian kernel random variable : (stats.gaussian_kde)
              when kernel is used
    &#34;&#34;&#34;
    mu = None
    sigma = None
    kde = None
    if fit_type == &#34;normal&#34;:
        # parametric, fit normal distribution
        logger.debug(&#34;parametric, fit normal distribution&#34;)

        # Fit a curve to the variates  mu is loc sigma is scale
        mu, sigma = stats.norm.fit(s, floc=s.mean())

    elif fit_type == &#34;kernel&#34;:
        # non-parametric, this creates the kernel, given an array it will estimate the probability over that values
        logger.debug(&#34;non-parametric kernel fit&#34;)
        s_dropna = s[~np.isnan(s)]  # remove nans
        # bandwidth selection:  gaussian_kde uses a rule of thumb, the default is Scott’s Rule.
        kde = stats.gaussian_kde(s_dropna)
    else:
        raise Exception(&#34;fit_type is not set correctly&#34;)

    if plot:
        if axn is None:
            axn = plt.gca()
        n = min(len(s) // 100, 100)  # bin size
        dist_space = np.linspace(min(s), max(s), 100)
        axn.hist(s, bins=n, density=True)

        # plot pdf
        if fit_type == &#34;normal&#34;:
            # probability distribution
            pdf = stats.norm.pdf(dist_space, mu, sigma)
            axn.plot(dist_space, pdf, label=&#34;normal&#34;)

        if fit_type == &#34;kernel&#34;:
            pdf_kde = kde(dist_space)
            axn.plot(dist_space, pdf_kde, label=&#34;kernel&#34;)

        axn.set_xlabel(xlabel)
        axn.set_ylabel(&#34;distribution density&#34;)
        axn.legend(loc=&#34;upper right&#34;)
        axn.set_title(title)
    # return
    if fit_type == &#34;normal&#34;:
        return stats.norm(loc=mu, scale=sigma)
    if fit_type == &#34;kernel&#34;:
        return kde</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.Get_mean"><code class="name flex">
<span>def <span class="ident">Get_mean</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>get mean ignoring nans</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Get_mean(x):
    &#34;&#34;&#34;get mean ignoring nans&#34;&#34;&#34;
    x = x[~np.isnan(x)]  # remove nans
    return x.mean()</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.Get_std"><code class="name flex">
<span>def <span class="ident">Get_std</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>get standard deviation ignoring nans</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Get_std(x):
    &#34;&#34;&#34;get standard deviation ignoring nans&#34;&#34;&#34;
    x = x[~np.isnan(x)]  # remove nans
    return x.std()</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.Hist_custom"><code class="name flex">
<span>def <span class="ident">Hist_custom</span></span>(<span>S)</span>
</code></dt>
<dd>
<div class="desc"><p>plot histogram with N_SAMPLE//100 bins ignoring nans</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Hist_custom(S):
    &#34;&#34;&#34;plot histogram with N_SAMPLE//100 bins ignoring nans&#34;&#34;&#34;
    S_dropna = S[~np.isnan(S)]
    fig, ax = plt.subplots()
    ax.hist(
        S_dropna, bins=min(N_SAMPLE // 100, 100), density=True, alpha=0.5, color=&#34;C0&#34;
    )</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.Normal_custom"><code class="name flex">
<span>def <span class="ident">Normal_custom</span></span>(<span>m, s, n_sample=100000, non_negative=False, plot=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Sampling from a normal distribution</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code> or <code>float</code></dt>
<dd>mean</dd>
<dt><strong><code>s</code></strong> :&ensp;<code>int</code> or <code>float</code></dt>
<dd>standard deviation</dd>
<dt><strong><code>n_sample</code></strong> :&ensp;<code>int</code></dt>
<dd>sample number, default is a Global var N_SAMPLE</dd>
<dt><strong><code>non_negative</code></strong> :&ensp;<code>bool</code></dt>
<dd>if true, return truncated distribution with no negatives, default is False</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code></dt>
<dd>default is False</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Normal_custom(m, s, n_sample=N_SAMPLE, non_negative=False, plot=False):
    &#34;&#34;&#34; Sampling from a normal distribution

    Parameters
    ----------
    m : int or float
        mean
    s : int or float
        standard deviation
    n_sample : int
        sample number, default is a Global var N_SAMPLE
    non_negative: bool
        if true, return truncated distribution with no negatives, default is False
    plot : bool
        default is False

    Returns
    -------
    out : numpy array
    &#34;&#34;&#34;
    x = np.random.normal(loc=m, scale=s, size=n_sample)
    if non_negative:
        x = stats.truncnorm.rvs(
            (0 - m) / s, (np.inf - m) / s, loc=m, scale=s, size=n_sample
        )
    if plot:
        fig, ax = plt.subplots()
        ax.hist(x)
        plt.show()
    return x</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.Pf_RS"><code class="name flex">
<span>def <span class="ident">Pf_RS</span></span>(<span>R_info, S, R_distrib_type='normal', plot=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the probability of failure
Pf = P(R-S&lt;0), given the R(resistance) and S(load)
with three three methods and use method 3) if checked OK with the other two
1) crude monte carlo
2) numerical integral of g kernel fit
3) R S integral: ('$\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$')
reliability index(beta factor) is calculated with simple 1st order g.mean()/g.std()</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>R_info</code></strong> :&ensp;<code>tuple</code> or <code>numpy array</code></dt>
<dd>distribution of Resistance, e.g. cover thickness, critical chloride content, tensile strength
can be array or distribution parameters
R_distrib_type='normal' -&gt; tuple(m,s) for normal m: mean s: standard deviation
R_distrib_type='normal' -&gt; tuple(m,s,a,b) for (General) beta distribution
m: mean, s: standard deviation a,b : lower, upper bound
R_distrib_type='normal' -&gt; array: for not-determined distribution, will be treated numerically(R S integral is not applied )</dd>
<dt><strong><code>S</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>distribution of load, e.g. carbonation depth, chloride content, tensile stress
the distribution type is calculated S is usually not determined, can vary a lot in different cases, therefore fitted with kernel</dd>
<dt><strong><code>R_distrib_type</code></strong> :&ensp;<code>string</code></dt>
<dd>'normal', 'beta', 'array'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>out = tuple</code></dt>
<dd>(probability of failure, reliability index)</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For R as arrays R S integral is not applied
R S integration method: $P_f = P(R-S&lt;=0)=\int\limits_{-\infty}^{\infty}f_S(y) \int\limits_{-\infty}^{y}f_R(x)dxdy$
the dual numerical integration seems too computationally expensive, so consider fit R to analytical distribution in the future versions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Pf_RS(R_info, S, R_distrib_type=&#34;normal&#34;, plot=False):  # updated!
    &#34;&#34;&#34;Calculate the probability of failure  Pf = P(R-S&lt;0), given the R(resistance) and S(load)
       with three three methods and use method 3) if checked OK with the other two
           1) crude monte carlo
           2) numerical integral of g kernel fit
           3) R S integral: (&#39;$\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$&#39;)
       reliability index(beta factor) is calculated with simple 1st order g.mean()/g.std()

    Parameters
    ----------
    R_info : tuple or numpy array
             distribution of Resistance, e.g. cover thickness, critical chloride content, tensile strength
             can be array or distribution parameters
             R_distrib_type=&#39;normal&#39; -&gt; tuple(m,s) for normal m: mean s: standard deviation
             R_distrib_type=&#39;normal&#39; -&gt; tuple(m,s,a,b) for (General) beta distribution
                             m: mean, s: standard deviation a,b : lower, upper bound
             R_distrib_type=&#39;normal&#39; -&gt; array: for not-determined distribution, will be treated numerically(R S integral is not applied )

    S : numpy array
        distribution of load, e.g. carbonation depth, chloride content, tensile stress
        the distribution type is calculated S is usually not determined, can vary a lot in different cases, therefore fitted with kernel

    R_distrib_type : string
        &#39;normal&#39;, &#39;beta&#39;, &#39;array&#39;

    Returns
    -------
    out = tuple
        (probability of failure, reliability index)

    Notes
    -----
    For R as arrays R S integral is not applied
    R S integration method: $P_f = P(R-S&lt;=0)=\int\limits_{-\infty}^{\infty}f_S(y) \int\limits_{-\infty}^{y}f_R(x)dxdy$
    the dual numerical integration seems too computationally expensive, so consider fit R to analytical distribution in the future versions

    &#34;&#34;&#34;
    from scipy import integrate

    R, pf_RS = (None, None)

    S_kde_fit = Fit_distrib(S, fit_type=&#34;kernel&#34;)
    S_dropna = S[~np.isnan(S)]

    if R_distrib_type == &#34;normal&#34;:
        # R = (mu, std)
        (m, s) = R_info
        R_distrib = stats.norm(m, s)
        R = R_distrib.rvs(size=N_SAMPLE)

        # Calculate probablility of failure
        #     $P_f = P(R-S&lt;=0)=\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$
        pf_RS = integrate.quad(
            lambda x: R_distrib.cdf(x) * S_kde_fit(x)[0], 0, S_dropna.max()
        )[0]

    elif R_distrib_type == &#34;beta&#34;:
        # R = (m, s, a, b) a, b are lower and upper bound
        (m, s, a, b) = R_info

        # location:c and scale:d for General Beta (standard Beta range [0,1])
        # calculate loc and scale
        c = a
        d = b - a

        # mean and variance for
        mu = (m - c) / d
        var = s ** 2 / d ** 2

        # shape params for Z~standard beta
        alpha = ((1 - mu) / var - 1 / mu) * mu ** 2
        beta = alpha * (1 / mu - 1)

        R_distrib = stats.beta(alpha, beta, c, d)
        R = R_distrib.rvs(size=N_SAMPLE)

        # Calculate probability of failure
        #     $P_f = P(R-S&lt;=0)=\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$
        pf_RS = integrate.quad(
            lambda x: R_distrib.cdf(x) * S_kde_fit(x)[0], 0, S_dropna.max()
        )[0]

    elif R_distrib_type == &#34;array&#34;:
        # dual numerical integration is too expensive, consider fit R to analytical distribution!!!!!!!!!!!!!!!!!!!!!!!!!!
        # plot condition to be fixed !!!!!!!!!!!!!!!!!!!!!!

        #         # use R array
        #         R_kde_fit = Fit_distrib(R, fit_type=&#39;kernel&#39;)
        #         R_dropna = R[~np.isnan(R)]
        #         # $P_f = P(R-S&lt;=0)=\int\limits_{-\infty}^{\infty}f_S(y) \int\limits_{-\infty}^{y}f_R(x)dxdy$

        #         def R_cdf_S_pdf(x, R_kde_fit, S_kde_fit):
        #             R_cdf = integrate.quad(lambda z: R_kde_fit(z)[0],0,x)[0] # kde_fit returns ([array needed]). therefore use lamda z kde(z)[0]
        #             S_pdf = S_kde_fit(x)[0]
        #             return R_cdf*S_pdf

        #         pf_RS = integrate.quad(R_cdf_S_pdf,0,S_dropna.max(), args=(R_kde_fit, S_kde_fit))[0]
        R_distrib = None
    else:
        R_distrib = None
        pass

    # compare with
    # numerical g
    g = R - S
    g = g[~np.isnan(g)]
    # numerical kernel fit
    g_kde_fit = Fit_distrib(g, fit_type=&#34;kernel&#34;, plot=False)
    pf_kde = integrate.quad(g_kde_fit, g.min(), 0)[0]
    pf_sample = len(g[g &lt;= 0]) / len(g)
    beta_factor = g.mean() / g.std()  # first order

    # check for tiny tail
    if pf_sample &lt; 1e-10:
        print(&#34;warning: very small Pf &#34;)
        logger.warning(&#34;warning: very small Pf &#34;)

    # check if pf_RS is the pf (should be)
    best_2_of_3 = find_similar_group([pf_sample, pf_kde, pf_RS], similar_group_size=2)
    if pf_RS not in best_2_of_3:
        logger.warning(&#34;warning: pf_RS is not used, double check&#34;)
        logger.warning(
            &#34;Pf(g = R-S &lt; 0) from various methods\n    sample count: {}\n    g integral: {}\n    R S integral: {}\n    beta_factor: {}&#34;.format(
                pf_sample, pf_kde, pf_RS, beta_factor
            )
        )

    logger.info(
        &#34;Pf(g = R-S &lt; 0) from various methods\n    sample count: {}\n    g integral: {}\n    R S integral: {}\n    beta_factor: {}&#34;.format(
            pf_sample, pf_kde, pf_RS, beta_factor
        )
    )

    if plot:
        print(&#34;Pf(g = R-S &lt; 0) from various methods&#34;)
        print(&#34;    sample count: {}&#34;.format(pf_sample))
        print(&#34;    g integral: {}&#34;.format(pf_kde))
        print(&#34;    R S integral: {}&#34;.format(pf_RS))
        # printmd(&#39;$\int\limits_{-\infty}^{\infty} F_R(x)f_S(x)dx$&#39;)
        print(&#34;    beta_factor: {}&#34;.format(beta_factor))

        # Plot R S
        fig, [ax1, ax2] = plt.subplots(ncols=2, figsize=(10, 3))
        # R
        R_plot = np.linspace(R.min(), R.max(), 100)
        ax1.plot(R_plot, R_distrib.pdf(R_plot), color=&#34;C0&#34;)
        ax1.hist(
            R,
            bins=min(N_SAMPLE // 100, 100),
            density=True,
            alpha=0.5,
            color=&#34;C0&#34;,
            label=&#34;R&#34;,
        )

        # S
        S_plot = np.linspace(S_dropna.min(), S_dropna.max(), 100)
        ax1.plot(S_plot, S_kde_fit(S_plot), color=&#34;C1&#34;, alpha=1)
        ax1.hist(
            S_dropna,
            bins=min(N_SAMPLE // 100, 100),
            density=True,
            alpha=0.5,
            color=&#34;C1&#34;,
            label=&#34;S&#34;,
        )

        ax1.set_title(
            &#34;S: mean = {:.1f} stdev = {:.1f}&#34;.format(S_dropna.mean(), S_dropna.std())
        )
        ax1.legend()
        plt.tight_layout()

        # plot g
        g_plot = np.linspace(g.min(), g.max(), 100)
        ax2.plot(g_plot, g_kde_fit(g_plot), color=&#34;C2&#34;, alpha=1)

        ax2.hist(
            g,
            density=True,
            bins=min(N_SAMPLE // 100, 100),
            color=&#34;C2&#34;,
            alpha=0.5,
            label=&#34;g=R-S&#34;,
        )
        ax2.vlines(x=0, ymin=0, ymax=g_kde_fit(0)[0], linestyles=&#34;--&#34;, alpha=0.5)
        ax2.vlines(
            x=g.mean(), ymin=0, ymax=g_kde_fit(g.mean())[0], linestyles=&#34;--&#34;, alpha=0.5
        )
        #         ax.annotate(s=&#39;&#39;, xy=(0,g_kde_fit(0)[0]), xytext=(g.mean(),g_kde_fit(0)[0]),
        #                     arrowprops={&#39;arrowstyle&#39;: &#39;&lt;-&gt;&#39;},va=&#39;center&#39;)
        ax2.annotate(
            s=r&#34;${\mu}_g$&#34;,
            xy=(0, g.mean()),
            xytext=(g.mean(), g_kde_fit(0)[0]),
            va=&#34;center&#34;,
        )
        ax2.legend()
        ax2.set_title(&#34;Limit-state P(g&lt;0)={}&#34;.format(pf_RS))
        plt.show()

    return pf_RS, beta_factor, R_distrib, S_kde_fit</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.RS_plot"><code class="name flex">
<span>def <span class="ident">RS_plot</span></span>(<span>model, ax=None, t_offset=0, amplify=1)</span>
</code></dt>
<dd>
<div class="desc"><p>plot R S distribution vertically at a time to an axis</p>
<h2 id="parameters">Parameters</h2>
<p>model.R_distrib : scipy.stats._continuous_distns, normal or beta
calculated in Pf_RS() through model.postproc()
model.S_kde_fit : stats.gaussian_kde
calculated in Pf_RS() through model.postproc()
distribution of load, e.g. carbonation depth, chlride content, tensile
stress. The distrubtion type is calculated S is usually not determined, can vary a lot in different cases, therefore fitted with kernel</p>
<dl>
<dt>model.S : numpy array</dt>
<dt>load, e.g. carbonation depth, chloride content, tensile stress</dt>
<dt><strong><code>ax</code></strong> :&ensp;<code>axis</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>t_offset</code></strong> :&ensp;<code>time offset to move the plot along the t-axis. default is zero</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>amplify</code></strong> :&ensp;<code>scale the height</code> of <code>the pdf plot</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def RS_plot(model, ax=None, t_offset=0, amplify=1):  # updated!
    &#34;&#34;&#34;plot R S distribution vertically at a time to an axis

    Parameters
    ----------
    model.R_distrib : scipy.stats._continuous_distns, normal or beta
                      calculated in Pf_RS() through model.postproc()
    model.S_kde_fit : stats.gaussian_kde
                      calculated in Pf_RS() through model.postproc()
                      distribution of load, e.g. carbonation depth, chlride content, tensile     stress. The distrubtion type is calculated S is usually not determined, can vary a lot in different cases, therefore fitted with kernel

    model.S : numpy array
              load, e.g. carbonation depth, chloride content, tensile stress
    ax : axis
    t_offset : time offset to move the plot along the t-axis. default is zero
    amplify : scale the height of the pdf plot
    &#34;&#34;&#34;

    R_distrib = model.R_distrib
    S_kde_fit = model.S_kde_fit
    S = model.S

    S_dropna = S[~np.isnan(S)]
    # Plot R S
    R = R_distrib.rvs(size=N_SAMPLE)

    if ax is None:
        ax = plt.gca()
    # R
    R_plot = np.linspace(R.min(), R.max(), 100)
    ax.plot(R_distrib.pdf(R_plot) * amplify + t_offset, R_plot, color=&#34;C0&#34;)
    ax.fill_betweenx(
        R_plot,
        t_offset,
        R_distrib.pdf(R_plot) * amplify + t_offset,
        color=&#34;C0&#34;,
        alpha=0.5,
        label=&#34;R&#34;,
    )
    # S  avoid ploting large S with very small probability
    S_plot = np.linspace(S_dropna.min(), min(5 * S_dropna.mean(), S_dropna.max()), 100)
    ax.plot(S_kde_fit(S_plot) * amplify + t_offset, S_plot, color=&#34;C1&#34;, alpha=1)
    ax.fill_betweenx(
        S_plot,
        t_offset,
        S_kde_fit(S_plot) * amplify + t_offset,
        color=&#34;C1&#34;,
        alpha=0.5,
        label=&#34;S&#34;,
    )</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.dropna"><code class="name flex">
<span>def <span class="ident">dropna</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dropna(x):
    return x[~np.isnan(x)]</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.f_solve_poly2"><code class="name flex">
<span>def <span class="ident">f_solve_poly2</span></span>(<span>a, b, c)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def f_solve_poly2(a, b, c):
    r1 = (-b + (b ** 2 - 4 * a * c) ** 0.5) / (2 * a)
    r2 = (-b - (b ** 2 - 4 * a * c) ** 0.5) / (2 * a)
    return r1, r2</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.find_mean"><code class="name flex">
<span>def <span class="ident">find_mean</span></span>(<span>val, s, confidence_one_tailed=0.95)</span>
</code></dt>
<dd>
<div class="desc"><p>return the mean value of a unknown normal distribution
based on the given value at a known one-tailed confidence level(default 95%)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>val</code></strong> :&ensp;<code>float</code></dt>
<dd>cut-off value</dd>
<dt><strong><code>s</code></strong> :&ensp;<code>standard deviation</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>confidence_one_tailed</code></strong> :&ensp;<code>confidence level</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>mean value</code> of <code>the unknown distribution</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_mean(val, s, confidence_one_tailed=0.95):
    &#34;&#34;&#34;return the mean value of a unknown normal distribution
    based on the given value at a known one-tailed confidence level(default 95%)

    Parameters
    ----------
    val : float
         cut-off value
    s : standard deviation
    confidence_one_tailed : confidence level

    Returns
    -------
    mean : mean value of the unknown distribution
    &#34;&#34;&#34;

    def func(m, s, val, confidence_one_tailed):
        &#34;&#34;&#34;object function to be solved&#34;&#34;&#34;
        norm = stats.norm(m, s)
        cutoff = norm.cdf(val)
        return cutoff - (1 - confidence_one_tailed)

    from scipy.optimize import fsolve

    mean = fsolve(func, x0=val, args=(s, val, confidence_one_tailed))[
        0
    ]  # use val as initial guess
    return mean</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.find_similar_group"><code class="name flex">
<span>def <span class="ident">find_similar_group</span></span>(<span>item_list, similar_group_size=2)</span>
</code></dt>
<dd>
<div class="desc"><p>find similar sublist of similar_group_size from a item_list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_similar_group(item_list, similar_group_size=2):
    &#34;&#34;&#34;find similar sublist of similar_group_size from a item_list&#34;&#34;&#34;
    from itertools import combinations

    combos = np.array(list(combinations(item_list, similar_group_size)))
    ind_min = combos.std(axis=1).argmin()
    similar_group = combos[ind_min].tolist()
    return similar_group</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.interp_extrap_f"><code class="name flex">
<span>def <span class="ident">interp_extrap_f</span></span>(<span>x, y, x_find, plot=False)</span>
</code></dt>
<dd>
<div class="desc"><p>interpolate or extrapolate value from an array with fitted2-deg or 3-deg polynomial</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>array-like</code></dt>
<dd>varible</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code></dt>
<dd>function value</dd>
<dt><strong><code>x_find</code></strong> :&ensp;<code>int</code> or <code>float</code> or <code>array-like</code></dt>
<dd>look-up x</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code></dt>
<dd>plot curve fit and data points, default if false</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code> or <code>float</code> or <code>array-like</code></dt>
<dd>inter/extrapolated value(s), raise warning when extrapolation is used</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interp_extrap_f(x, y, x_find, plot=False):
    &#34;&#34;&#34;interpolate or extrapolate value from an array with fitted2-deg or 3-deg polynomial

    Parameters
    ----------
    x : array-like
        varible
    y : array-like
        function value
    x_find : int or float or array-like
        look-up x
    plot : bool
        plot curve fit and data points, default if false

    Returns
    -------
    int or float or array-like
        inter/extrapolated value(s), raise warning when extrapolation is used
    &#34;&#34;&#34;

    def func2(x, a, b, c):
        # 2-order polynomial
        return a * (x ** 2) + b * (x ** 1) + c

    def func3(x, a, b, c, d):
        # 3-order polynomial
        return a * (x ** 2) + b * (x ** 2) + c * x + d

    if (x_find &lt; x.min()).any() or (x_find &gt; x.max()).any():
        logger.warning(&#34;Warning: extrapolation used&#34;)

    from scipy.optimize import curve_fit

    # Initial parameter guess, just to kick off the optimization
    if len(y) &gt; 3:
        logger.debug(&#34;use func3: 3-deg polynomial&#34;)
        guess = (0.5, 0.5, 0.5, 0.5)
        popt, _ = curve_fit(func3, x, y, p0=guess)
        y_find = func3(x_find, *popt)
        if plot:
            fig, ax = plt.subplots()
            ax.plot(x, y, &#34;.&#34;, label=&#34;table&#34;)
            _plot_data = np.linspace(x.min(), x.max(), 100)
            ax.plot(_plot_data, func3(_plot_data, *popt), &#34;--&#34;)
            ax.plot(
                x_find, y_find, &#34;x&#34;, color=&#34;r&#34;, markersize=8, label=&#34;interp/extrap data&#34;
            )
            ax.legend()
            plt.show()

    elif len(y) &lt;= 3:
        logger.debug(&#34;use func2: 2-deg polynomial&#34;)
        guess = (0.5, 0.5, 0.5)
        popt, _ = curve_fit(func2, x, y, p0=guess)
        y_find = func2(x_find, *popt)
        if plot:
            fig, ax = plt.subplots()
            ax.plot(x, y, &#34;.&#34;, label=&#34;table&#34;)
            _plot_data = np.linspace(x.min(), x.max(), 100)
            ax.plot(_plot_data, func2(_plot_data, *popt), &#34;--&#34;)
            ax.plot(
                x_find, y_find, &#34;x&#34;, color=&#34;r&#34;, markersize=8, label=&#34;interp/extrap data&#34;
            )
            ax.legend()
            plt.show()
    else:
        y_find = None
    return y_find</code></pre>
</details>
</dd>
<dt id="Tinkrete.helper_func.sample_integral"><code class="name flex">
<span>def <span class="ident">sample_integral</span></span>(<span>Y, x)</span>
</code></dt>
<dd>
<div class="desc"><p>integrate Y over x, where every Y point is a bunch of distribution samples,</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>Y</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>2D
column: y data points
row: samples for y point</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>int_y_x</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>integral of y over x for all sampled data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_integral(Y, x):
    &#34;&#34;&#34;integrate Y over x, where every Y point is a bunch of distribution samples,

    Parameters
    ----------
    Y : numpy array
        2D
        column: y data points
        row: samples for y point
    x : numpy array

    Returns
    -------
    int_y_x : numpy array
            integral of y over x for all sampled data
    &#34;&#34;&#34;
    from scipy.integrate import simps

    n, _ = Y.shape
    if n != len(x):
        raise Exception(&#34;Y does not have the same number of data points as x&#34;)
    int_y_x = simps(Y, x, axis=0)
    return int_y_x</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Tinkrete" href="index.html">Tinkrete</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="Tinkrete.helper_func.Beta_custom" href="#Tinkrete.helper_func.Beta_custom">Beta_custom</a></code></li>
<li><code><a title="Tinkrete.helper_func.Fit_distrib" href="#Tinkrete.helper_func.Fit_distrib">Fit_distrib</a></code></li>
<li><code><a title="Tinkrete.helper_func.Get_mean" href="#Tinkrete.helper_func.Get_mean">Get_mean</a></code></li>
<li><code><a title="Tinkrete.helper_func.Get_std" href="#Tinkrete.helper_func.Get_std">Get_std</a></code></li>
<li><code><a title="Tinkrete.helper_func.Hist_custom" href="#Tinkrete.helper_func.Hist_custom">Hist_custom</a></code></li>
<li><code><a title="Tinkrete.helper_func.Normal_custom" href="#Tinkrete.helper_func.Normal_custom">Normal_custom</a></code></li>
<li><code><a title="Tinkrete.helper_func.Pf_RS" href="#Tinkrete.helper_func.Pf_RS">Pf_RS</a></code></li>
<li><code><a title="Tinkrete.helper_func.RS_plot" href="#Tinkrete.helper_func.RS_plot">RS_plot</a></code></li>
<li><code><a title="Tinkrete.helper_func.dropna" href="#Tinkrete.helper_func.dropna">dropna</a></code></li>
<li><code><a title="Tinkrete.helper_func.f_solve_poly2" href="#Tinkrete.helper_func.f_solve_poly2">f_solve_poly2</a></code></li>
<li><code><a title="Tinkrete.helper_func.find_mean" href="#Tinkrete.helper_func.find_mean">find_mean</a></code></li>
<li><code><a title="Tinkrete.helper_func.find_similar_group" href="#Tinkrete.helper_func.find_similar_group">find_similar_group</a></code></li>
<li><code><a title="Tinkrete.helper_func.interp_extrap_f" href="#Tinkrete.helper_func.interp_extrap_f">interp_extrap_f</a></code></li>
<li><code><a title="Tinkrete.helper_func.sample_integral" href="#Tinkrete.helper_func.sample_integral">sample_integral</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>